---
title: "Lab 5 MATH 342W"
author: "Sergio E. Garcia Tapia"
output: pdf_document
date: "11:59PM March 16"
---

Load up the diamonds data

```{r}
pacman::p_load(ggplot2, skimr)
diamonds = ggplot2::diamonds[1:1000, ]
```

Consider the regression of price on color. Construct the design matrix with an intercept, X1 using `model.matrix` and its corresponding hat matrix H1.

```{r}
X1 = model.matrix(price ~ color, data = diamonds)
dim(X1)
```

To construct $H_1$ from $X_1$, we can use the formula we learned in lecture

$$
H_1 = X_1(X_1^\top X_1)^{-1}X_1^\top
$$

```{r}
hat_mat = function(X) {
  X %*% solve(t(X) %*% X) %*% t(X)
}

H1 = hat_mat(X1)
```

Now construct the design matrix without the intercept, X2 using `model.matrix` and its corresponding hat matrix H2.

```{r}
# Drop intercept by using 0 + color
# The column space is the same in the case of nominal categories
# So you can do this because we are dealing with nominal categories
# In general it's not recommended to drop the intercept
# By default, intercept is not dropped, and model.matrix does the
# dummification in our behalf.

X2 = model.matrix(price ~ 0 + color, data = diamonds)
H2 = hat_mat(X2)
```

Verify the hat matrix constructed from this design matrix is the same as the hat matrix constructed from the design matrix with the intercept. (Fact: orthogonal projection matrices are unique).

```{r}
pacman::p_load(testthat)
# No output means the are equal.
expect_equal(H1, H2)
```

Write a function spec'd as follows:

```{r}
#' Orthogonal Projection
#'
#' Projects vector `a` onto the subspace spanned by `v`.
#'
#' @param a   The vector we are projecting.
#' @param v   The vector whose subspace we are projecting onto.
#'
#' @returns   a list of two vectors, the orthogonal projection parallel to v
#'            named `a_parallel`,  and the orthogonal error orthogonal to v
#'            called `a_perpendicular`.
orthogonal_projection = function(a, v){
  if (length(a) != length(v)) {
    stop("The vectors must be of equal length")
  }
  
  proj_coeff = sum(v * a) / sum(v * v)
  a_parallel = proj_coeff * v
  
  list(a_parallel = a_parallel, a_perpendicular = a - a_parallel)
}
```

Provide predictions for each of these computations and then run them to make sure you're correct.

```{r}
orthogonal_projection(c(1,2,3,4), c(1,2,3,4))
# prediction: a_parallel=(1, 2, 3, 4), a_perpendicular=(0, 0, 0, 0) because it is
# projected onto itself.
orthogonal_projection(c(1, 2, 3, 4), c(0, 2, 0, -1))
# prediction: a_parallel=(0, 0, 0, 0), a_perpendicular=(1, 2, 3, 4) because the
# vectors are already orthogonal.
result = orthogonal_projection(c(2, 6, 7, 3), c(1, 3, 5, 7))
t(result$a_parallel) %*% result$a_perpendicular
# prediction: (0, 0, 0, 0), because a_parallel + a_perpendicular is an orthogonal
# decomposition of (2, 6, 7, 3) when orthogonally projected onto (1, 3, 5, 7).
result$a_parallel + result$a_perpendicular
# prediction: (2, 6, 7, 3), because the a_parallel and a_perpendicular make up
# an orthogonal decomposition of (2, 6, 7, 3) when orthogonally projected
# onto (1, 3, 5, 7)
# scalar-wise division, so you get the constant for each entry
result$a_parallel / c(1, 3, 5, 7)
# prediction: A vector of the form k * (1, 1, 1, 1), where k represents
# the constant needed to write a_parallel as k(1, 3, 5, 7). This is because
# a_parallel is in the span of (1, 3, 5, 7), so the vectorized division will
# yield the same constant for all entries
```

Using the Boston housing data and create a design matrix X for all features and vector y for the responses.

```{r}
y = MASS::Boston$medv
# Last column is the response, medv
p = ncol(MASS::Boston) - 1
X = cbind(1, as.matrix(MASS::Boston[, 1 : p]))
```

Find the OLS solution for X being used as the design matrix.

The OLS solution is given by

$$
\mathbf{b} = (X^\top X)^{-1} X^\top\mathbf{y}
$$
The prediction is given by

$$
\mathbf{\hat{y}} = X\mathbf{b}
$$

```{r}
ols_sol = function(X, y) {
  solve(t(X) %*% X) %*% t(X) %*% y
}

b = ols_sol(X, y)
yhat = X %*% b
```

Using your function `orthogonal_projection` from the earlier, orthogonally project onto the column space of X by projecting y on each column of X individually and adding up the projections and call the sum `yhat_naive`.

```{r}
yhat_naive = rep(0, length(yhat))
for (j in 1 : ncol(X)) {
  yhat_naive = yhat_naive + orthogonal_projection(y, X[, j])$a_parallel
}
rm(j)
```

How much double counting occurred? Measure the magnitude relative to the true LS orthogonal projection.

```{r}
# If the columns of X are not already orthogonal, you get a projection in the col space that has a different length.
# This will measure how much off it is (how much over or undercounting)
# if the answer is 1, the it is an orthogonal projection
sqrt(sum(yhat_naive^2)) / sqrt(sum(yhat^2))
```

Is this ratio expected? Why or why not?

Not the exact value, but the fact that it is above 1. If the columns of $X$ are 
not already orthogonal, then the orthogonal projection of a vector onto the
column space of $X$ will not be equal to the sum of the orthogonal projections
onto the one-dimensional subspaces spanned by each column.

Convert X into V where V has the same column space as X but has orthogonal columns. You can use the function `orthogonal_projection`. This is the Gram-Schmidt orthogonalization algorithm (part A).

```{r}
# Base step: first orthogonal vector is the first vector in the list.
V = matrix(NA, nrow = nrow(X), ncol = ncol(X))
V[, 1] = X[, 1]

# Iterative step: the next orthogonal vector is obtained by removing the
# orthogonal projection of the current vector onto each of the
# previously built orthogonal vectors.
for (j in 2 : ncol(X)){
  V[, j] = X[, j]
  for (k in 1 : (j-1)) {
    V[, j] = V[, j] - orthogonal_projection(X[, j], V[, k])$a_parallel
  }
}
rm(j, k)
```

Convert V into Q whose columns are the same except normalized. This is the Gram-Schmidt orthogonalization algorithm (part B).

```{r}
Q = matrix(NA, nrow = nrow(X), ncol = ncol(X))
for(j in 1 : ncol(X)){
  Q[, j] = V[, j] / sqrt(sum(V[, j]^2))
}
rm(V, j)
```

Verify that Q\^T Q = I\_{p+1} i.e. Q is an orthonormal matrix.

```{r}
expect_equal(t(Q) %*% Q, diag(ncol(Q)))
```

Is your Q the same as what results from R's built-in QR-decomposition function?

```{r}
qr_obj = qr(X)
Q_from_r = qr.Q(qr_obj)
expect_equal(as.numeric(Q_from_r), as.numeric(Q))
```

Is this expected? Why did this happen?

The orthogonal vectors in the QR decomposition are not necessarily unique.
For example, in an orthogonal list $q_0,q_1,\ldots,q_p$, we can change
$q_0$ by $q_0' = -q_0$, and the new list will still be orthogonal.

Project the y vector onto each column of the Q matrix and test if the sum of these projections is the same as yhat.

```{r}
# No output indicates success.
expect_equal(as.numeric(yhat), as.numeric(Q %*% t(Q) %*% y))
```

Find the OLS estimates b_Q if Q is used as the design matrix and compare to b, the estimates from the original design matrix X.

The OLS estimates can be computed with the formula:

$$
\mathbf{b}_Q = (Q^\top Q)^{-1}Q^\top\mathbf{y}
$$
Since $Q$ is an orthonormal matrix, $Q^\top Q=I_{p+1}$, so this reduces to

$$
\mathbf{b}_Q = Q^\top \mathbf{y}
$$
```{r}
b_Q = t(Q) %*% y
cbind(b, b_Q)
expect_equal(as.numeric(b), as.numeric(b_Q))
```

Are b and b_Q the same? Why or why not?

No. $\mathbf{b}$ consists of the coefficients needed to express the projection
of $\mathbf{y}$ (that is, $\mathbf{\hat{y}}$) as a linear combination of the
columns of $X$. Similarly, $\mathbf{b}_Q$ consists of the coefficients needed
to express the $\mathbf{\vec{y}}$ as a linear combination of the columns of $Q$.
Since the lists are distinct, the coefficients will generally differ. This
is in spite of the fact that the column space of $X$ and $Q$ is the same,
and that we are projecting the same vector $\mathbf{y}$ and obtaning the
same vector for the projection, $\mathbf{\hat{y}}$.

Ensure that the predicted values are the same for both linear models: the one created with X as its design matrix and the one created with Q as its design matrix.

```{r}
expect_equal(as.numeric(X %*% b), as.numeric(Q %*% b_Q))
```

Clear the workspace and load the boston housing data and extract X and y. The dimensions are n = 506 and p = 13. Create a matrix that is (p + 1) x (p + 1) full of NA's. Label the columns the same columns as X. Do not label the rows.

For the first row, find the OLS estimate of the y regressed on the first column only and put that in the first entry. For the second row, find the OLS estimates of the y regressed on the first and second columns of X only and put them in the first and second entries. For the third row, find the OLS estimates of the y regressed on the first, second and third columns of X only and put them in the first, second and third entries, etc. For the last row, fill it with the full OLS estimates.

```{r}
rm(list = setdiff(ls(), c("hat_mat", "ols_sol")))
X = as.matrix(cbind(1,MASS::Boston[, 1:13]))
y = MASS::Boston$medv

n = nrow(X)
p_plus_1 = ncol(X)

# We are doing p_plus_1 regressions
ls_estimates = matrix(data = NA, nrow = p_plus_1, ncol = p_plus_1)
for (j in 1:p_plus_1) {
  X_j = X[, 1:j, drop=FALSE]
  ls_estimates[j ,1:j] = ols_sol(X_j, y)
}
round(ls_estimates, 3)
rm(j)
```

Why are the estimates changing from row to row as you add in more predictors?

This occurs because the columns of $X$ are not orthogonal. If we were using
an orthonormal matrix, $Q$, then the coefficients would not change because in
that case, the projection of the sums is equal to the sum of the projections.

Create a vector of length p+1 and compute the R\^2 values for each of the above models.

$R^2$ is given by

$$
R^2 = 1 - \frac{SSE}{SST}
$$
Therefore, we need to compute the residuals $\mathbf{e}$ to compute the $SSE$.
We can do this by using $I-H$, which is the orthogonal projection matrix onto
the residual space.

```{r}
results = array(NA, p_plus_1)
for (j in 1:p_plus_1) {
  X_j = X[, 1:j, drop=FALSE]
  e = (diag(n) - hat_mat(X_j)) %*% y
  results[j] = sum(e^2)
}
rm(j)
SST = (n-1) * var(y)
round(1 - results / SST, 3)
```

Is R\^2 monotonically increasing? Why?

Yes, because with each iteration, we add an extra column corresponding to a new
predictor, and the design matrix is closer to filling the entire space.
The value would not increase if the new predictor were orthogonal to the
response vector $\mathbf{y}$, but the probability of that occurring is very
small.

Create a 2x2 matrix with the first column 1's and the next column iid normal realizations. Find the absolute value of the angle (in degrees, not radians) between the two columns in absolute difference from 90 degrees.

```{r}
X = cbind(1, rnorm(2))
angle_between = function(u, v) {
  if (length(u) != length(v)) {
    stop("The vectors must have equal length")
  }
  acos(sum(u * v) / (sqrt(sum(u^2) * sum(v^2)))) * 180 / pi
}
angle_between(X[, 1], X[, 2])
```

Repeat this exercise `Nsim = 1e5` times and report the average absolute angle.

```{r}
Nsim = 1e5
abs_angles = array(data = NA, dim = Nsim)
for (nsim in 1:Nsim) {
  X = cbind(1, rnorm(2))
  abs_angles[nsim] = abs(angle_between(X[, 1], X[, 2]))
}
mean(abs_angles)
```

Create a n x 2 matrix with the first column 1's and the next column iid normal realizations. Find the absolute value of the angle (in degrees, not radians) between the two columns. For n = 10, 50, 100, 200, 500, 1000, report the average absolute angle over `Nsim = 1e5` simulations.

```{r}
mean_angle_by_trial_size = cbind(
  c(10, 50, 100, 200, 500, 1000),
  NA
)
for (i in 1:nrow(mean_angle_by_trial_size)) {
  n = mean_angle_by_trial_size[i, 1]
  abs_angles = array(data = NA, dim = Nsim)
  for (nsim in 1:Nsim) {
    X = cbind(1, rnorm(2))
    abs_angles[nsim] = abs(angle_between(X[, 1], X[, 2]))
  }
  mean_angle_by_trial_size[i, 2] = mean(abs_angles)
}
mean_angle_by_trial_size
```

What is this absolute angle difference from 90 degrees converging to? Why does this make sense?

Note that the arccosine is between $0^\circ$ and $180^\circ$ so on average it
seems reasonable that the angle between two random vectors falls halfway between
$0^\circ$ and $180^\circ$, and hence that it should be close to $90^\circ$.

Create a vector y by simulating n = 100 standard iid normals. Create a matrix of size 100 x 2 and populate the first column by all ones (for the intercept) and the second column by 100 standard iid normals. Find the R\^2 of an OLS regression of `y ~ X`. Use matrix algebra.

```{r}
n = 100
y = rnorm(n)
X = cbind(1, rnorm(n))
SST = sum((y - mean(y))^2)
e = (diag(n) - hat_mat(X)) %*% y
SSE = sum(e^2)
Rsq = 1 - SSE / SST
Rsq
```

Write a for loop to each time bind a new column of 100 standard iid normals to the matrix X and find the R\^2 each time until the number of columns p+1 = n = 100. Create a vector to save all R\^2's. What happened??

```{r}
Rsqs = array(n)
SST = sum((y - mean(y))^2)
for (j in 1:(n-2)) {
  X = cbind(X, rnorm(n))
  e = (diag(n) - hat_mat(X)) %*% y
  SSE = sum(e^2)
  Rsqs[j] = 1 - SSE / SST
}
Rsqs
```
The $R^2$ error metric eventually becomes $1$ because when $X$ becomes a full
rank $n\times n$ matrix, thus covering the entire space and yielding a perfect
fit.

Test that the projection matrix onto this X is the same as I_n. You may have to vectorize the matrices in the `expect_equal` function for the test to work.

```{r}
pacman::p_load(testthat)
expect_equal(hat_mat(X), diag(ncol(X)))
```

Add one final column to X to bring the number of columns to 101. Then try to compute R\^2. What happens?

```{r}
X = cbind(X, rnorm(n))
SST = sum((y - mean(y))^2)
e = (diag(ncol(X)) - hat_mat(X)) %*% y
SSE = sum(e^2)
Rsq = 1 - SSE / SST
Rsq
```

Why does this make sense?

Since $X$ now has 1 more column than rows overdetermined because it has more
columns than rows, and hence it is not full rank. Put another way, $X$ now
corresponds to a linear transformation from a vector from a vector space of
dimension $101$ to a vector space of dimension $100$, so it is not injective.
Hence, $X^\top X$ is not injective. This causes the `solve()` function that
attempts to compute the inverse of $X^\top X$ in the computation of the hat
matrix $H$ to fail.

Split the Boston Housing Data into a training set and a test set where the training set is 80% of the observations. Do so at random.

```{r}
set.seed(37)
n = nrow(MASS::Boston)
K = 5
n_test = round(n * 1 / K)
n_train = n - n_test
idx_train = sample(1:n, size = n_train)
idx_test = setdiff(1:n, idx_train)
boston_train = MASS::Boston[idx_train,]
boston_test = MASS::Boston[idx_test,]
# We have pulled two chunks; 1/5 train, 4/5 test
```

Fit an OLS model. Find the s_e in sample and out of sample. Which one is greater? Note: we are now using s_e and not RMSE since RMSE has the n-(p + 1) in the denominator not n-1 which attempts to de-bias the error estimate by inflating the estimate when overfitting in high p. Again, we're just using `sd(e)`, the sample standard deviation of the residuals.

```{r}
mod = lm(medv ~ ., data=boston_train)
sd(mod$residuals)
yhat = predict(mod, boston_test)
sd(boston_test$medv - yhat)
```

The out-of-sample $S_e$ is greater.

Do these two exercises `Nsim = 1000` times and find the average difference between s_e and ooss_e.

```{r}
set.seed(314159)
K = 5
n = nrow(MASS::Boston)
n_test = round(n * 1 / K)
n_train = n - n_test

Nsim = 1000
se_in_minus_se_out = array(data = NA, dim = Nsim)
for (sim_id in 1 : Nsim) {
  idx_train = sample(1:n, size = n_train)
  idx_test = setdiff(1:n, idx_train)
  boston_train = MASS::Boston[idx_train,]
  boston_test = MASS::Boston[idx_test,]
  
  mod = lm(medv ~ ., data=boston_train)
  yhat = predict(mod, boston_test)
  
  se_in_minus_se_out[sim_id] = sd(mod$residuals) - sd(boston_test$medv - yhat)
}
mean(se_in_minus_se_out)
```

On average, `s_e` is lower than `ooss_e`, so the average difference is negative.

We'll now add random junk to the data so that `p_plus_one = n_train` and create a new data matrix `X_with_junk.`

```{r}
X = cbind(1, boston_train[, 1: 13])
p_plus_one = ncol(X)
X_with_junk = cbind(X, matrix(rnorm(n_train * (n_train - p_plus_one)), nrow = nrow(X)))
dim(X)
dim(X_with_junk)
```

Repeat the exercise above measuring the average s_e and ooss_e but this time record these metrics by number of features used. That is, do it for the first column of `X_with_junk` (the intercept column), then do it for the first and second columns, then the first three columns, etc until you do it for all columns of `X_with_junk`. Save these in `s_e_by_p` and `ooss_e_by_p`.

```{r}
s_e_by_p = array(data = NA, dim = n_train)
ooss_e_by_p = array(data = NA, dim = n_train)
for (j in 1: n_train) {
  # Model using training set, through column j
  mod = lm(boston_train$medv ~ 0 + ., data = X_with_junk[, 1:j, drop=FALSE])
  
  # in-sample error
  s_e_by_p[j] = sd(mod$residuals)
  
  y_hat = as.matrix(cbind(1, boston_test[, 1:13])) %*% mod$coefficients[1:p_plus_one]
  
  # out-of-sample error
  ooss_e_by_p[j] = sd(boston_test$medv - y_hat)
}
```

You can graph them here:

```{r}
pacman::p_load(ggplot2)
ggplot(
  rbind(
    data.frame(s_e = s_e_by_p, p = 1 : n_train, series = "in-sample"),
    data.frame(s_e = ooss_e_by_p, p = 1 : n_train, series = "out-of-sample")
  )) +
  geom_line(aes(x = p, y = s_e, col = series))
```

Is this shape expected? Explain.

Yes, because the "random junk" that we added is just fitting the noise;
it has nothing to do with our features. The in-sample error goes to zero
because the junk matrix becomes full rank and becomes a perfect fit.
However, predictions are worse, a demonstrated by the out-of-sample blowing
up. This is because the least squares estimates for the features that matter
become worse as we add junk features, and the junk features skew our
predictions.

Now repeat the exercise above except use 5-fold CV (K=5 cross validation) for each p. The code below will also plot the oos RMSE. This oos RMSE curve should be similar to the curve in the above problem, but now it will be more stable.

```{r}
K = 5
ooss_e_by_p_k = matrix(NA, nrow = n, ncol = n) #save all residuals here - each row are the residuals for number of features = j

print(length(MASS::Boston$medv[idx_train]))
p = p_plus_one - 1
for (j in (p_plus_one + 1): ncol(X_with_junk)) {
  for (k in 1:K) {
    idx_test = seq(from=(k-1) * n_test + 1, to = min(n, k * n_test))
    idx_train = setdiff(1:n, idx_test)
    
    X_train = X_with_junk[idx_train, 1:j]
    y_train = MASS::Boston$medv[idx_train]
    
    X_test = X_with_junk[idx_test, 1:p]
    y_test = MASS::Boston$medv[idx_test]
    
    # Fit linear model with training set
    mod = lm(y_train ~ 0 + as.matrix(X_train))
    
    # Use model to compute predictions on the test set
    yhat = as.matrix(cbind(1, X_test)) %*% mod$coefficients[1:p_plus_one]
    
    ooss_e_by_p_k[j, idx_test] = y_test - yhat
  }
}

#now plot it
pacman::p_load(ggplot2)
ggplot(data.frame(
    s_e = apply(ooss_e_by_p_k, 1, sd, na.rm = TRUE), #we are taking the sd over all n oos residuals
    p = 1 : ncol(ooss_e_by_p_k)
  )) +
  geom_line(aes(x = p, y = s_e))

```

Even though the concept of confidence intervals (CIs) will not be on the midterm, construct 95% CIs for each of the oosRMSE measurements by number of features, p. A CI is a real-number interval with a lower bound and upper bound. The formula for the CI is [s_e - 2 \* s_s_e, s_e + 2 \* s_s_e].

```{r}
#TODO
```
