---
title: "Lab 7"
author: "Sergio E. Garcia Tapia"
output: pdf_document
---

Let's load up the diamonds dataset and compute ln_price (not raw price) and consider it as the prediction target. 

```{r}
rm(list=ls())
pacman::p_load(ggplot2) #this loads the diamonds data set too
diamonds = ggplot2::diamonds
?diamonds
diamonds$cut =      factor(diamonds$cut, ordered = FALSE)      #convert to nominal
diamonds$color =    factor(diamonds$color, ordered = FALSE)    #convert to nominal
diamonds$clarity =  factor(diamonds$clarity, ordered = FALSE)  #convert to nominal
diamonds$ln_carat = log(diamonds$carat)
diamonds$ln_price = log(diamonds$price)
```

Create model (A) of ln_price ~ ln_carat.

```{r}
#Model A
mod_a = lm(ln_price ~ ln_carat, diamonds)
summary(mod_a)$sigma
summary(mod_a)$r.squared
```

Create a model (B) of ln_price on ln_carat interacted with clarity and compare its performance with the model (A).

```{r}
#Model B
mod_b = lm(ln_price ~ ln_carat * clarity, diamonds)
summary(mod_b)$sigma
summary(mod_b)$r.squared
```

Which model does better? Why?

Model B is better because the $R^2$ metric is closer to 1 and its RMSE
is smaller. The RMSE units for both models are logarithmic price, so the RMSEs
are comparable.

Create a model of (C) ln_price on ln_carat interacted with every categorical feature (clarity, cut and color) and compare its performance with model (B)

```{r}
#Model C
mod_c = lm(ln_price ~ ln_carat * (clarity + cut + color), diamonds)
summary(mod_c)$sigma
summary(mod_c)$r.squared
```

Which model does better? Why?

Model C is better. We see a further increase in $R^2$ and decrease in RMSE.

Create a model (D) of ln_price on every continuous feature (logs of carat, x, y, z, depth, table) interacted with every categorical feature (clarity, cut and color) and compare its performance with model (C).

```{r}
#Model D
mod_d = lm(ln_price ~ (ln_carat + x + y + z + depth + table) * (clarity + cut + color), diamonds)
summary(mod_d)$sigma
summary(mod_d)$r.squared
```

Which model does better? Why?

Model D is better. We see a further increase in $R^2$ and decrease in RMSE.

What is the p of this model D? Compute with code.

```{r}
?lm
p = length((mod_d)$coefficients) - 1
p
```

Create model (E) which is the same as before except create include the raw features interacted with the categorical features and gauge the performance against (D).

```{r}
#Model E
mod_e = lm(ln_price ~ (carat + x + y + z + depth + table) * (clarity + cut + color), diamonds)
summary(mod_e)$sigma
summary(mod_e)$r.squared
```

Which model does better? Why?

Model E is worse. The value of $R^2$ is not as close to $1$, and the RMSE
increases.

Create model (F) which is the same as before except also include also third degree polynomials of the continuous features interacted with the categorical features and gauge performance against (E). By this time you're getting good with R's formula syntax!

```{r}
#Model F
mod_f = lm(ln_price ~ (carat + x + y + z + depth + table +
                         poly(carat, 3) +poly(x, 3) + poly(y, 3) + poly(z, 3) + poly(depth, 3) + poly(table, 3)) * 
             (clarity + cut + color), diamonds)
summary(mod_f)$sigma
summary(mod_f)$r.squared
```

Which model does better? Why?

Model F does better than all previous models.

Can you think of any other way to expand the candidate set curlyH? Discuss.

We could add quadratic transformed features second-degree interactions,
and logarithmic transformations of continuous features.

We should probably assess oos performance now. Sample 2,000 diamonds and use these to create a training set of 1,800 random diamonds and a test set of 200 random diamonds. Define K and do this splitting:

```{r}
#TO-DO
set.seed(1984)
n_sub = 2000
n = nrow(diamonds)
K = n_sub / 200
n_test = ceiling(n_sub / K)
n_train = n_sub - n_test
test_idx = sample(1 : n, n_test)
train_idx = sample(setdiff(1:n, test_idx), n_train)
```

Compute in and out of sample performance for models A-F. Use s_e as the metric (standard error of the residuals). Create a list with keys A, B, ..., F to store these metrics. Remember the performances here will be worse than before since before you're using nearly 52,000 diamonds to build a model and now it's only 1,800! 

```{r}
#TO-DO
model_formulas = list(
  A = ln_price ~ ln_carat,
  B = ln_price ~ ln_carat * clarity,
  C = ln_price ~ ln_carat * (clarity + cut + color),
  D = ln_price ~ (ln_carat + x + y + z + depth + table) * (clarity + cut + color),
  E = ln_price ~ (carat + x + y + z + depth + table) * (clarity + cut + color),
  F = ln_price ~ (carat + x + y + z + depth + table +
                         poly(carat, 3) +poly(x, 3) + poly(y, 3) + poly(z, 3) + poly(depth, 3) + poly(table, 3)) * 
             (clarity + cut + color)
)

oos_se = list()
all_models_train = list()
for (model_idx in LETTERS[1 : 6]){
  all_models_train[[model_idx]] = lm(model_formulas[[model_idx]], diamonds[train_idx, ])
  summary(all_models_train[[model_idx]])$sigma
  oos_se[[model_idx]] = sd(diamonds$ln_price[test_idx] - predict(all_models_train[[model_idx]], diamonds[test_idx, ]))
}
oos_se
```

You computed oos metrics only on n_* = 200 diamonds. What problem(s) do you expect in these oos metrics?

The oos metrics will be unstable (meaning high in variance).


To do the K-fold cross validation we need to get the splits right and crossing is hard. We've developed code for this already in a previous lab.

```{r}
?rnorm
set.seed(1984)
temp = rnorm(n_sub)
folds_vec = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
rm(temp)
head(folds_vec, 200)
length(folds_vec)
```

Do the K-fold cross validation for model F and compute the overall s_e and s_s_e. 

```{r}
#TO-DO
e_vec_k = list() #for each one
for (k in 1 : K){
  test_indicies_k = which(folds_vec == k)
  train_indicies_k = which(folds_vec != k)
  mod = lm(model_formulas[['F']], diamonds[train_indicies_k, ])
  e_vec_k[[k]] = sd(diamonds$ln_price[test_indicies_k] - predict(mod, diamonds[test_indicies_k, ]))
}
oos_se = mean(unlist(e_vec_k)) #note: not exactly the overall sd, but close enough
oos_s_se = sd(unlist(e_vec_k))

oos_se
oos_s_se
```

Does K-fold CV help reduce variance in the oos s_e? Discuss.

Yes, because each fold trains the model on a different portion of the overall
data set, capturing some of the variance of the out-of-sample RMSE. By averaging
the RMSEs, that variance is smoothed out (decreased).

Imagine using the entire rest of the dataset besides the 2,000 training observations divvied up into slices of 200. Measure the oos error for each slice on Model F in a vector `s_e_s_F` and compute the `s_s_e_F` and also plot it.

```{r}
all_other_indicies = setdiff(1 : n, c(test_idx, train_idx))
n_step = 1 / K * n_sub
starting_ks = seq(from = 1, to = (length(all_other_indicies) - n_step), by = n_step)
s_e_s_F = list() #for each one
for (k in 1 : length(starting_ks)){
  diamonds_k = diamonds[all_other_indicies[starting_ks[k] : (starting_ks[k] + n_step - 1)], ]
  s_e_s_F[[k]] = sd(diamonds_k$ln_price - predict(all_models_train[['F']], diamonds_k))
}
pacman::p_load(ggplot2) #this loads the diamonds data set too
s_e_s_F = unlist(s_e_s_F)
ggplot(data.frame(s_e_s_F = s_e_s_F)) + geom_histogram(aes(x = s_e_s_F))
```


#Model Selection with Three Splits: Select from M models

We employ the diamonds dataset and specify M models nested from simple to more complex. We store the models as strings in a list (i.e. a hashset)

```{r}
rm(list=ls())
?ggplot2::diamonds
# These are the M pre-specified models as discussed in lecture.
model_formulas = c(
  "carat",
  "carat + cut",
  "carat + cut + color",
  "carat + cut + color + clarity",
  "carat + cut + color + clarity + x + y + z",
  "carat + cut + color + clarity + x + y + z + depth",
  "carat + cut + color + clarity + x + y + z + depth + table",
  "carat * (cut + color + clarity) + x + y + z + depth + table",
  "(carat + x + y + z) * (cut + color + clarity) + depth + table",
  "(carat + x + y + z + depth + table) * (cut + color + clarity)",
  "(poly(carat, 2) + x + y + z + depth + table) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + depth + table) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth) + log(table)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth) + log(table)) * (cut + color + clarity + poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth) + log(table))"
)
model_formulas = paste0("price ~ ", model_formulas)
M = length(model_formulas)
```

In order to use the formulas with logs we need to eliminate rows with zeros in those measurements:

```{r}
diamonds_cleaned = ggplot2::diamonds
diamonds_cleaned = diamonds_cleaned[
  diamonds_cleaned$carat > 0 &
  diamonds_cleaned$x > 0 &
  diamonds_cleaned$y > 0 &
  diamonds_cleaned$z > 0 &
  diamonds_cleaned$depth > 0 &
  diamonds_cleaned$table > 0, #all columns
]
```

Split the data into train, select and test. Each set should have 1/3 of the total data.

```{r}
n = nrow(diamonds_cleaned)
set.seed(1)
train_idx = sample(1 : n, round(n / 3))
select_idx = sample(setdiff(1 : n, train_idx), round(n / 3))
test_idx = setdiff(1 : n, c(train_idx, select_idx))
diamonds_train =  diamonds_cleaned[train_idx, ]
diamonds_select = diamonds_cleaned[select_idx, ]
diamonds_test =   diamonds_cleaned[test_idx, ]
```

Find the oosRMSE on the select set for each model. Save the number of df in each model while you're doing this as we'll need it for later.

```{r}
# Each of the M models uses a different number of features.
# dfs keeps track of the number of features in each.
dfs = array(NA, M)

# RMSE computed for the mth model when trained on D_train and
# tested on D_select
oosRMSEs = array(NA, M)

n_select = nrow(diamonds_select)

# For each model...
for (m in 1:M) {
  # Fit the mth model on the fixed D_train set
  mod = lm(model_formulas[m], data = diamonds_train)
  
  # Compute oos error for mth model on D_select
  yhat = predict(mod, diamonds_select)
  e = diamonds_select$price - yhat
  oosRMSEs[m] = sqrt(sum(e^2) / n_select)
  
  dfs[m] = mod$rank
}
```

Plot the oosRMSE by model complexity (df in model)

```{r}
pacman::p_load(ggplot2)
ggplot(data.frame(df = dfs, oosRMSE = oosRMSEs)) +
    geom_point(aes(x = df, y = oosRMSE), size = 3) +
    geom_line(aes(x = df, y = oosRMSE)) +
    ylim(1, 1e5) + scale_y_log10()
```

Select the best model by oosRMSE and find its oosRMSE on the test set.

```{r}
# Model for which the oosRMSE was the loewst
m_star = which.min(oosRMSEs)
cat("m_star=", m_star, "\n")
cat("g_{m_star}=", model_formulas[m_star], "\n")
cat("degrees of freedom for g_{m_star}=", dfs[m_star], "\n")
cat("oosRMSE_{m_star}=", oosRMSEs[m_star], "\n")

# Fit the m_star model which had the lowest RMSE on D_train union D_select
mod = lm(model_formulas[m_star], rbind(diamonds_train, diamonds_select))

# Compute the oosRMSE of g_{m_star} that was trained on D_train union D_select
# by predicting on D_test
yhat = predict(mod, diamonds_test)
e = diamonds_test$price - yhat
cat("oosRMSE=", sqrt(sum(e^2) / nrow(diamonds_test)), "\n")

# Comparing the oosRMSE of g_{m_star} on D_train
# versus
# oosRMSE of g_{m_star} on D_train union D_select
# helps us see how well g_{m_star} does out-of-sample, and hence how
# well we will predict in the future.

# If the oosRMSE on the D_select alone is smaller than the oosRMSE on
# D_test, then we have overfit.

```

Did we overfit the select set? Discuss why or why not.

**Solution**: No, because the out-of-sample RMSE on $\mathbb{D}_{select} for
$g_{m^*}$ is larger than the out-of-sample RMSE on $\mathbb{D}_{test}$. Had it
been higher, than we would have overfit.


Create the final model object `g_final`.

```{r}
# We train g_{m_star} on all of D, which is the union of train, select, and test
# We could have simply done diamonds_cleaned, but this is more explicit.
mod = lm(model_formulas[m_star], rbind(diamonds_train, diamonds_select, diamonds_test))
```


#Model Selection with Three Splits: Hyperparameter selection

We will use an algorithm that I historically taught in 324W but now moved to 343 so I can teach it more deeply using the Bayesian topics from 341. The regression algorithm is called "ridge" and it involves solving for the slope vector via:

b_ridge := (X^T X + lambda I_(p+1))^-1 X^T y

Note how if lambda = 0, this is the same algorithm as OLS. If lambda becomes very large then b_ridge is pushed towards all zeroes. So ridge is good at weighting only features that matter.

However, lambda is a hyperparameter >= 0 that needs to be selected.

We will work with the boston housing dataset except we will add 250 garbage features consisting of iid N(0,1) realizations. We will also standardize the columns so they're all xbar = 0 and s_x = 1. This is shown to be important in 343.

```{r}
rm(list = ls())
?MASS::Boston
y = MASS::Boston$medv
X = model.matrix(medv ~ ., MASS::Boston)
n = nrow(X)
p_garbage = 250
set.seed(1)
X = cbind(X, matrix(rnorm(n * p_garbage), nrow = n))
X = apply(X, 2, function(x_dot_j){
                  (x_dot_j - mean(x_dot_j)) / sd(x_dot_j)
                })
X[, 1] = 1 #we standardized the intercept column which became zeroes - make it an intercept again
dim(X)
```


Now we split it into 300 train, 100 select and 106 test. 

```{r}
set.seed(1)
train_idx = sample(1 : n, 300)
select_idx = sample(setdiff(1 : n, train_idx), 100)
test_idx = setdiff(1 : n, c(train_idx, select_idx))

X_train = X[train_idx,]
X_select = X[select_idx,]
X_test = X[test_idx,]

y_train = y[train_idx]
y_select = y[select_idx]
y_test = y[test_idx]
```

We now create a grid of M = 200 models indexed by lambda. The lowest lambda should be zero (which is OLS) and the highest lambda can be 100.

```{r}
M = 200
lambda_grid = seq(from = 5, to = 1000, length.out = M)
```

Now find the oosRMSE on the select set on all models each with their own lambda value.

```{r}
#TO-DO
oos_rmses = array(NA, M)
p_plus_one = ncol(X_train)
for (m in 1 : length(lambda_grid)) {
  lambda = lambda_grid[m]
  b_ridge_lambda = solve(t(X_train) %*% X_train + lambda * diag(p_plus_one)) %*% t(X_train) %*% y_train
  y_hat_select = X_select %*% b_ridge_lambda
  oos_rmses[m] = sd(y_select - y_hat_select)
}
```

Plot the oosRMSE by the value of lambda.

```{r}
pacman::p_load(ggplot2)
ggplot(data.frame(oos_rmses = oos_rmses)) +
  geom_point(aes(x= 1 : M, y = oos_rmses)) + scale_y_log10()
```

Select the model with the best oosRMSE on the select set and find its oosRMSE on the test set.

```{r}
#TO-DO
# Do at home
m_star = which.min(oos_rmses)
lambda = lambda_grid[m_star]
X_train_select = rbind(X_train, X_select)
y_train_select = c(y_train, y_select)
b_ridge_lambda = solve(t(X_train_select) %*% X_train_select + lambda * diag(p_plus_one)) %*% t(X_train_select) %*% y_train_select
y_hat_test = X_test %*% b_ridge_lambda
oosRMSE_test = sd(y_test - y_hat_test)
oosRMSE_test
```

Create the final model object `g_final`.

```{r}
Xfull = rbind(X_train, X_select, X_test) # or just use X
yfull = c(y_train, y_select, y_test)
b_ridge_lambda = solve(t(Xfull) %*% Xfull + lambda * diag(p_plus_one)) %*% t(Xfull) %*% yfull

g_final = function(X_future) {
  X_future %*% b_ridge_lambda
}
```


#Model Selection with Three Splits: Forward stepwise modeling

We will use the adult data

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult) #remove any observations with missingness
n = nrow(adult)
?adult
#let's remove "education" as its duplicative with education_num
adult$education = NULL
```


To implement forward stepwise, we need a "full model" that contains anything and everything we can possible want to use as transformed predictors. Let's first create log features of all the numeric features. Instead of pure log, use log(value + 1) to handle possible zeroes.

```{r}
#this gives us the list of numeric features to create logs
skimr::skim(adult)

adult$log_age = log(adult$age + 1)
adult$log_fnlwgt = log(adult$fnlwgt + 1)
adult$log_education_num = log(adult$education_num + 1)
adult$log_capital_gain = log(adult$capital_gain + 1)
adult$log_capital_loss = log(adult$capital_loss + 1)
adult$log_hours_per_week = log(adult$hours_per_week + 1)
```

Now let's create a model matrix Xfull that contains all first order interactions. How many degrees of freedom in this "full model"?

```{r}
#TO-DO
head(adult)
# We want to take all possible 19 features and we want to interact them with
# each other. For example x1x2, x1x3, ..., x1x_{19}
# . * . is like a cartesian product to get all the interactions
# It also accounts for interactions with itself, like x_1^2, x_2^2,...
Xfull = model.matrix(income ~ . * ., adult)
dim(Xfull)
```

Now let's split it into train, select and test sets. Because this will be a glm, model-building (training) will be slow, so let's keep the training set small at 2,000. Since prediction is fast, we can divide the others evenly among select and test.

```{r}
y = ifelse(adult$income == ">50K", 1, 0)
p_plus_one  = ncol(Xfull)

n_sub = 2000
train_idx = sample(1 : nrow(adult), n_sub)
select_idx = sample(setdiff(1 : nrow(adult), train_idx), n_sub)
test_idx = sample(setdiff(1:nrow(adult), c(train_idx, select_idx)), n_sub)

Xfull_train =  Xfull[train_idx, ]
Xfull_select = Xfull[select_idx, ]
Xfull_test =   Xfull[test_idx, ]
y_train =      y[train_idx]
y_select =     y[select_idx]
y_test =       y[test_idx]
```

Now let's use the code from class to run the forward stepwise modeling. As this is binary classification, let's use logistic regression and to measure model performance, let's use the Brier score. Compute the Brier score in-sample (on training set) and oos (on selection set) for every iteration of j, the number of features selected from the greedy selection procedure.

```{r}
#TO-DO
included_features_by_iter = c() #keep a growing list of predictors by iteration
in_sample_brier_by_iteration = c() #keep a growing list of se's by iteration
oos_brier_by_iteration = c() #keep a growing list of se's by iteration
i = 1

repeat {

  #get all predictors left to try
  all_briers = array(NA, p_plus_one) #record all possibilities
  for (j_try in 1 : p_plus_one){
    if (j_try %in% included_features_by_iter){
      next 
    }
    Xmm_sub = Xfull_train[, c(included_features_by_iter, j_try), drop = FALSE]
    logistic_mod = suppressWarnings(glm(y_train ~ 0 + ., data = data.frame(Xmm_sub), family = binomial("logit")))
    phat = suppressWarnings(predict(logistic_mod, data.frame(Xmm_sub)))
    all_briers[j_try] = -mean((y_train - phat)^2)
  }
  j_star = which.max(all_briers)
  included_features_by_iter = c(included_features_by_iter, j_star)
  in_sample_brier_by_iteration = c(in_sample_brier_by_iteration, all_briers[j_star])
  
  #now let's look at oos
  Xmm_sub = Xfull_train[, included_features_by_iter, drop = FALSE]
  logistic_mod = suppressWarnings(glm(y_train ~ 0 + ., data = data.frame(Xmm_sub), family = binomial("logit")))
  phat_select = suppressWarnings(predict(logistic_mod, data.frame(Xfull_select[, included_features_by_iter, drop = FALSE])))
  oos_brier = -mean((y_select - phat_select)^2)
  oos_brier_by_iteration = c(oos_brier_by_iteration, oos_brier)
  
  cat("i =", i, "in sample: brier = ", round(all_briers[j_star], 1), "oos_se", round(oos_brier, 1), "added:", colnames(Xfull_train)[j_star], "\n")
  
  i = i + 1
  
  if (i > n_sub || i > p_plus_one){
    break #why??
  }
}
```

Plot the in-sample Brier score (in red) and oos Brier score (in blue) by the number of features used.

```{r}
results = data.frame(
  iteration = 1 : length(in_sample_brier_by_iteration),
  in_sample_brier_by_iteration = in_sample_brier_by_iteration,
  oos_brier_by_iteration = oos_brier_by_iteration
)

pacman::p_load(latex2exp)
ggplot(results) + 
  geom_point(aes(x = iteration, y = in_sample_brier_by_iteration), col = "red") +
  geom_point(aes(x = iteration, y = oos_brier_by_iteration), col = "blue") + 
  ylim(min(c(results$in_sample_brier_by_iteration, results$oos_brier_by_iteration)), 0)
  ylab(TeX("$s_e$"))
```

Select the model with the best oos Brier score on the select set and find its oos Brier score on the test set.

```{r}
best_features = included_features_by_iter[1: which.max(oos_brier_by_iteration)]
Xmm_sub = Xfull_train[, best_features, drop = FALSE]
logistic_mod = suppressWarnings(glm(y_train ~ 0 + ., data = data.frame(Xmm_sub), family = binomial("logit")))
phat_test = suppressWarnings(predict(logistic_mod, data.frame(Xfull_test[, best_features, drop = FALSE])))
oos_brier_test = -mean((y_test - phat_test)^2)
oos_brier_test
```

Create the final model object `g_final`.

```{r}
X_final = rbind(Xfull_train[, best_features, drop = FALSE], Xfull_select[, best_features, drop = FALSE], Xfull_select[, best_features, drop = FALSE])
y_final = c(y_train, y_select, y_test)
mod = suppressWarnings(glm(y_final ~ 0 + ., data = data.frame(X_final), family = binomial("logit")))
```
